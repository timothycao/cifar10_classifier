{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-09T23:54:10.352414Z",
     "iopub.status.busy": "2025-03-09T23:54:10.352229Z",
     "iopub.status.idle": "2025-03-09T23:54:11.323280Z",
     "shell.execute_reply": "2025-03-09T23:54:11.322539Z",
     "shell.execute_reply.started": "2025-03-09T23:54:10.352396Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_1\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_2\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/batches.meta\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/test_batch\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_3\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_5\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_4\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/readme.html\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:54:13.574834Z",
     "iopub.status.busy": "2025-03-09T23:54:13.574518Z",
     "iopub.status.idle": "2025-03-09T23:54:14.190069Z",
     "shell.execute_reply": "2025-03-09T23:54:14.189090Z",
     "shell.execute_reply.started": "2025-03-09T23:54:13.574807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cifar10_classifier'...\n",
      "remote: Enumerating objects: 93, done.\u001b[K\n",
      "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
      "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
      "remote: Total 93 (delta 53), reused 68 (delta 28), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (93/93), 24.05 KiB | 6.01 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "/kaggle/working/cifar10_classifier\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/timothycao/cifar10_classifier.git\n",
    "%cd cifar10_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:54:16.320457Z",
     "iopub.status.busy": "2025-03-09T23:54:16.320157Z",
     "iopub.status.idle": "2025-03-09T23:54:21.909653Z",
     "shell.execute_reply": "2025-03-09T23:54:21.908683Z",
     "shell.execute_reply.started": "2025-03-09T23:54:16.320435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 4848027\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from model import create_model\n",
    "from train import main as train\n",
    "from inference import main as inference\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "MODEL_NAME = 'ResNetCustom'\n",
    "BLOCK_TYPE = 'bottleneck'\n",
    "NUM_BLOCKS_PER_LAYER = [3, 4, 6, 3]\n",
    "NUM_CHANNELS_PER_LAYER = [29, 58, 116, 232]\n",
    "KERNEL_SIZE_PER_LAYER = [3, 3, 3, 3]\n",
    "SKIP_KERNEL_SIZE_PER_LAYER = [1, 1, 1, 1]\n",
    "EXPANSION = 4\n",
    "POOL_SIZE = 1\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 200\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 100\n",
    "AUGMENTATIONS = [\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=12),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.RandomGrayscale(p=0.2)\n",
    "]\n",
    "SAVE_MODE = 'best'  # Options: 'best', 'every'\n",
    "SAVE_EVERY_N = 1\n",
    "\n",
    "# Initialize model\n",
    "try:\n",
    "    model = create_model(\n",
    "        name=MODEL_NAME,\n",
    "        block_type=BLOCK_TYPE,\n",
    "        blocks_per_layer=NUM_BLOCKS_PER_LAYER,\n",
    "        channels_per_layer=NUM_CHANNELS_PER_LAYER,\n",
    "        kernels_per_layer=KERNEL_SIZE_PER_LAYER,\n",
    "        skip_kernels_per_layer=SKIP_KERNEL_SIZE_PER_LAYER,\n",
    "        expansion=EXPANSION,\n",
    "        pool_size=POOL_SIZE\n",
    "    )\n",
    "    print('Total model parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "except AssertionError as e:\n",
    "    print(f'Failed to create model: {e}')\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:54:26.699163Z",
     "iopub.status.busy": "2025-03-09T23:54:26.698664Z",
     "iopub.status.idle": "2025-03-10T03:19:19.512731Z",
     "shell.execute_reply": "2025-03-10T03:19:19.511705Z",
     "shell.execute_reply.started": "2025-03-09T23:54:26.699133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 4848027\n",
      "Loading data...\n",
      "Preprocessing pipeline:\n",
      " Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomCrop(size=(32, 32), padding=4)\n",
      "    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1))\n",
      "    RandomAffine(degrees=[-10.0, 10.0], translate=(0.1, 0.1), scale=(0.8, 1.2))\n",
      "    RandAugment(num_ops=2, magnitude=12, num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
      "    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))\n",
      "    RandomGrayscale(p=0.2)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initializing model...\n",
      "Training model...\n",
      "\n",
      "Epoch: 1/200      LR: 0.00200\n",
      "TRAIN: Loss: 2.16 Acc: 19.72%\n",
      "TEST:  Loss: 1.81 Acc: 32.49%\n",
      "\n",
      "Epoch: 2/200      LR: 0.00203\n",
      "TRAIN: Loss: 1.91 Acc: 29.61%\n",
      "TEST:  Loss: 1.81 Acc: 36.93%\n",
      "\n",
      "Epoch: 3/200      LR: 0.00214\n",
      "TRAIN: Loss: 1.76 Acc: 35.95%\n",
      "TEST:  Loss: 1.49 Acc: 46.53%\n",
      "\n",
      "Epoch: 4/200      LR: 0.00231\n",
      "TRAIN: Loss: 1.64 Acc: 41.03%\n",
      "TEST:  Loss: 1.37 Acc: 51.35%\n",
      "\n",
      "Epoch: 5/200      LR: 0.00254\n",
      "TRAIN: Loss: 1.52 Acc: 45.57%\n",
      "TEST:  Loss: 1.24 Acc: 55.21%\n",
      "\n",
      "Epoch: 6/200      LR: 0.00285\n",
      "TRAIN: Loss: 1.42 Acc: 49.69%\n",
      "TEST:  Loss: 1.13 Acc: 59.91%\n",
      "\n",
      "Epoch: 7/200      LR: 0.00321\n",
      "TRAIN: Loss: 1.34 Acc: 52.81%\n",
      "TEST:  Loss: 1.10 Acc: 61.96%\n",
      "\n",
      "Epoch: 8/200      LR: 0.00365\n",
      "TRAIN: Loss: 1.26 Acc: 55.66%\n",
      "TEST:  Loss: 0.99 Acc: 65.23%\n",
      "\n",
      "Epoch: 9/200      LR: 0.00414\n",
      "TRAIN: Loss: 1.21 Acc: 57.75%\n",
      "TEST:  Loss: 0.96 Acc: 67.53%\n",
      "\n",
      "Epoch: 10/200     LR: 0.00470\n",
      "TRAIN: Loss: 1.16 Acc: 59.39%\n",
      "TEST:  Loss: 0.95 Acc: 67.48%\n",
      "\n",
      "Epoch: 11/200     LR: 0.00532\n",
      "TRAIN: Loss: 1.11 Acc: 61.02%\n",
      "TEST:  Loss: 0.85 Acc: 71.12%\n",
      "\n",
      "Epoch: 12/200     LR: 0.00600\n",
      "TRAIN: Loss: 1.09 Acc: 62.26%\n",
      "TEST:  Loss: 0.94 Acc: 68.81%\n",
      "\n",
      "Epoch: 13/200     LR: 0.00673\n",
      "TRAIN: Loss: 1.05 Acc: 63.45%\n",
      "TEST:  Loss: 0.84 Acc: 71.78%\n",
      "\n",
      "Epoch: 14/200     LR: 0.00752\n",
      "TRAIN: Loss: 1.03 Acc: 64.09%\n",
      "TEST:  Loss: 0.75 Acc: 74.33%\n",
      "\n",
      "Epoch: 15/200     LR: 0.00837\n",
      "TRAIN: Loss: 1.00 Acc: 65.03%\n",
      "TEST:  Loss: 0.78 Acc: 73.79%\n",
      "\n",
      "Epoch: 16/200     LR: 0.00926\n",
      "TRAIN: Loss: 0.98 Acc: 65.90%\n",
      "TEST:  Loss: 0.80 Acc: 72.54%\n",
      "\n",
      "Epoch: 17/200     LR: 0.01020\n",
      "TRAIN: Loss: 0.97 Acc: 66.51%\n",
      "TEST:  Loss: 0.85 Acc: 72.33%\n",
      "\n",
      "Epoch: 18/200     LR: 0.01118\n",
      "TRAIN: Loss: 0.94 Acc: 67.45%\n",
      "TEST:  Loss: 0.74 Acc: 75.82%\n",
      "\n",
      "Epoch: 19/200     LR: 0.01221\n",
      "TRAIN: Loss: 0.94 Acc: 67.51%\n",
      "TEST:  Loss: 0.71 Acc: 75.90%\n",
      "\n",
      "Epoch: 20/200     LR: 0.01327\n",
      "TRAIN: Loss: 0.92 Acc: 68.01%\n",
      "TEST:  Loss: 0.63 Acc: 78.87%\n",
      "\n",
      "Epoch: 21/200     LR: 0.01437\n",
      "TRAIN: Loss: 0.91 Acc: 68.31%\n",
      "TEST:  Loss: 0.65 Acc: 77.70%\n",
      "\n",
      "Epoch: 22/200     LR: 0.01550\n",
      "TRAIN: Loss: 0.91 Acc: 68.58%\n",
      "TEST:  Loss: 0.72 Acc: 76.36%\n",
      "\n",
      "Epoch: 23/200     LR: 0.01667\n",
      "TRAIN: Loss: 0.90 Acc: 68.93%\n",
      "TEST:  Loss: 0.61 Acc: 79.57%\n",
      "\n",
      "Epoch: 24/200     LR: 0.01786\n",
      "TRAIN: Loss: 0.89 Acc: 69.42%\n",
      "TEST:  Loss: 0.68 Acc: 77.49%\n",
      "\n",
      "Epoch: 25/200     LR: 0.01907\n",
      "TRAIN: Loss: 0.88 Acc: 69.38%\n",
      "TEST:  Loss: 0.59 Acc: 79.77%\n",
      "\n",
      "Epoch: 26/200     LR: 0.02030\n",
      "TRAIN: Loss: 0.88 Acc: 69.29%\n",
      "TEST:  Loss: 0.77 Acc: 75.34%\n",
      "\n",
      "Epoch: 27/200     LR: 0.02155\n",
      "TRAIN: Loss: 0.87 Acc: 69.64%\n",
      "TEST:  Loss: 0.59 Acc: 79.59%\n",
      "\n",
      "Epoch: 28/200     LR: 0.02281\n",
      "TRAIN: Loss: 0.86 Acc: 70.27%\n",
      "TEST:  Loss: 0.56 Acc: 81.28%\n",
      "\n",
      "Epoch: 29/200     LR: 0.02409\n",
      "TRAIN: Loss: 0.86 Acc: 69.80%\n",
      "TEST:  Loss: 0.66 Acc: 77.43%\n",
      "\n",
      "Epoch: 30/200     LR: 0.02536\n",
      "TRAIN: Loss: 0.86 Acc: 70.28%\n",
      "TEST:  Loss: 0.62 Acc: 79.21%\n",
      "\n",
      "Epoch: 31/200     LR: 0.02664\n",
      "TRAIN: Loss: 0.85 Acc: 70.44%\n",
      "TEST:  Loss: 0.70 Acc: 77.39%\n",
      "\n",
      "Epoch: 32/200     LR: 0.02791\n",
      "TRAIN: Loss: 0.84 Acc: 70.82%\n",
      "TEST:  Loss: 0.73 Acc: 75.28%\n",
      "\n",
      "Epoch: 33/200     LR: 0.02919\n",
      "TRAIN: Loss: 0.85 Acc: 70.90%\n",
      "TEST:  Loss: 0.63 Acc: 79.24%\n",
      "\n",
      "Epoch: 34/200     LR: 0.03045\n",
      "TRAIN: Loss: 0.85 Acc: 70.88%\n",
      "TEST:  Loss: 0.60 Acc: 79.79%\n",
      "\n",
      "Epoch: 35/200     LR: 0.03170\n",
      "TRAIN: Loss: 0.84 Acc: 71.02%\n",
      "TEST:  Loss: 0.63 Acc: 79.24%\n",
      "\n",
      "Epoch: 36/200     LR: 0.03293\n",
      "TRAIN: Loss: 0.84 Acc: 71.07%\n",
      "TEST:  Loss: 0.60 Acc: 79.86%\n",
      "\n",
      "Epoch: 37/200     LR: 0.03414\n",
      "TRAIN: Loss: 0.84 Acc: 70.99%\n",
      "TEST:  Loss: 0.58 Acc: 80.23%\n",
      "\n",
      "Epoch: 38/200     LR: 0.03533\n",
      "TRAIN: Loss: 0.84 Acc: 70.83%\n",
      "TEST:  Loss: 0.60 Acc: 79.69%\n",
      "\n",
      "Epoch: 39/200     LR: 0.03650\n",
      "TRAIN: Loss: 0.84 Acc: 71.17%\n",
      "TEST:  Loss: 0.68 Acc: 77.58%\n",
      "\n",
      "Epoch: 40/200     LR: 0.03763\n",
      "TRAIN: Loss: 0.84 Acc: 70.91%\n",
      "TEST:  Loss: 0.71 Acc: 76.46%\n",
      "\n",
      "Epoch: 41/200     LR: 0.03873\n",
      "TRAIN: Loss: 0.83 Acc: 71.23%\n",
      "TEST:  Loss: 0.78 Acc: 74.47%\n",
      "\n",
      "Epoch: 42/200     LR: 0.03979\n",
      "TRAIN: Loss: 0.84 Acc: 70.99%\n",
      "TEST:  Loss: 0.60 Acc: 79.61%\n",
      "\n",
      "Epoch: 43/200     LR: 0.04082\n",
      "TRAIN: Loss: 0.84 Acc: 71.29%\n",
      "TEST:  Loss: 0.59 Acc: 79.33%\n",
      "\n",
      "Epoch: 44/200     LR: 0.04180\n",
      "TRAIN: Loss: 0.84 Acc: 71.07%\n",
      "TEST:  Loss: 0.65 Acc: 78.52%\n",
      "\n",
      "Epoch: 45/200     LR: 0.04274\n",
      "TRAIN: Loss: 0.84 Acc: 71.12%\n",
      "TEST:  Loss: 0.56 Acc: 80.91%\n",
      "\n",
      "Epoch: 46/200     LR: 0.04363\n",
      "TRAIN: Loss: 0.84 Acc: 71.28%\n",
      "TEST:  Loss: 0.68 Acc: 77.35%\n",
      "\n",
      "Epoch: 47/200     LR: 0.04448\n",
      "TRAIN: Loss: 0.84 Acc: 71.14%\n",
      "TEST:  Loss: 0.62 Acc: 79.07%\n",
      "\n",
      "Epoch: 48/200     LR: 0.04527\n",
      "TRAIN: Loss: 0.83 Acc: 71.38%\n",
      "TEST:  Loss: 0.63 Acc: 78.64%\n",
      "\n",
      "Epoch: 49/200     LR: 0.04600\n",
      "TRAIN: Loss: 0.83 Acc: 71.25%\n",
      "TEST:  Loss: 0.64 Acc: 77.80%\n",
      "\n",
      "Epoch: 50/200     LR: 0.04668\n",
      "TRAIN: Loss: 0.82 Acc: 71.64%\n",
      "TEST:  Loss: 0.63 Acc: 79.92%\n",
      "\n",
      "Epoch: 51/200     LR: 0.04730\n",
      "TRAIN: Loss: 0.83 Acc: 71.35%\n",
      "TEST:  Loss: 0.66 Acc: 78.10%\n",
      "\n",
      "Epoch: 52/200     LR: 0.04786\n",
      "TRAIN: Loss: 0.82 Acc: 71.70%\n",
      "TEST:  Loss: 0.53 Acc: 82.02%\n",
      "\n",
      "Epoch: 53/200     LR: 0.04835\n",
      "TRAIN: Loss: 0.82 Acc: 71.74%\n",
      "TEST:  Loss: 0.69 Acc: 75.97%\n",
      "\n",
      "Epoch: 54/200     LR: 0.04879\n",
      "TRAIN: Loss: 0.83 Acc: 71.38%\n",
      "TEST:  Loss: 0.60 Acc: 79.44%\n",
      "\n",
      "Epoch: 55/200     LR: 0.04915\n",
      "TRAIN: Loss: 0.82 Acc: 71.71%\n",
      "TEST:  Loss: 0.76 Acc: 74.80%\n",
      "\n",
      "Epoch: 56/200     LR: 0.04946\n",
      "TRAIN: Loss: 0.82 Acc: 71.73%\n",
      "TEST:  Loss: 0.63 Acc: 78.41%\n",
      "\n",
      "Epoch: 57/200     LR: 0.04969\n",
      "TRAIN: Loss: 0.82 Acc: 71.81%\n",
      "TEST:  Loss: 0.78 Acc: 74.35%\n",
      "\n",
      "Epoch: 58/200     LR: 0.04986\n",
      "TRAIN: Loss: 0.82 Acc: 71.79%\n",
      "TEST:  Loss: 0.66 Acc: 77.74%\n",
      "\n",
      "Epoch: 59/200     LR: 0.04997\n",
      "TRAIN: Loss: 0.82 Acc: 71.74%\n",
      "TEST:  Loss: 0.60 Acc: 79.65%\n",
      "\n",
      "Epoch: 60/200     LR: 0.05000\n",
      "TRAIN: Loss: 0.82 Acc: 71.83%\n",
      "TEST:  Loss: 0.70 Acc: 76.74%\n",
      "\n",
      "Epoch: 61/200     LR: 0.04999\n",
      "TRAIN: Loss: 0.81 Acc: 72.07%\n",
      "TEST:  Loss: 0.64 Acc: 78.83%\n",
      "\n",
      "Epoch: 62/200     LR: 0.04997\n",
      "TRAIN: Loss: 0.81 Acc: 72.28%\n",
      "TEST:  Loss: 0.63 Acc: 78.91%\n",
      "\n",
      "Epoch: 63/200     LR: 0.04994\n",
      "TRAIN: Loss: 0.81 Acc: 72.17%\n",
      "TEST:  Loss: 0.60 Acc: 79.94%\n",
      "\n",
      "Epoch: 64/200     LR: 0.04990\n",
      "TRAIN: Loss: 0.81 Acc: 72.16%\n",
      "TEST:  Loss: 0.71 Acc: 76.23%\n",
      "\n",
      "Epoch: 65/200     LR: 0.04984\n",
      "TRAIN: Loss: 0.80 Acc: 72.22%\n",
      "TEST:  Loss: 0.65 Acc: 78.17%\n",
      "\n",
      "Epoch: 66/200     LR: 0.04977\n",
      "TRAIN: Loss: 0.80 Acc: 72.33%\n",
      "TEST:  Loss: 0.54 Acc: 81.49%\n",
      "\n",
      "Epoch: 67/200     LR: 0.04969\n",
      "TRAIN: Loss: 0.80 Acc: 72.71%\n",
      "TEST:  Loss: 0.60 Acc: 80.45%\n",
      "\n",
      "Epoch: 68/200     LR: 0.04960\n",
      "TRAIN: Loss: 0.80 Acc: 72.48%\n",
      "TEST:  Loss: 0.53 Acc: 81.69%\n",
      "\n",
      "Epoch: 69/200     LR: 0.04949\n",
      "TRAIN: Loss: 0.79 Acc: 72.75%\n",
      "TEST:  Loss: 0.56 Acc: 80.47%\n",
      "\n",
      "Epoch: 70/200     LR: 0.04937\n",
      "TRAIN: Loss: 0.80 Acc: 72.53%\n",
      "TEST:  Loss: 0.55 Acc: 81.42%\n",
      "\n",
      "Epoch: 71/200     LR: 0.04924\n",
      "TRAIN: Loss: 0.80 Acc: 72.74%\n",
      "TEST:  Loss: 0.58 Acc: 79.89%\n",
      "\n",
      "Epoch: 72/200     LR: 0.04910\n",
      "TRAIN: Loss: 0.80 Acc: 72.51%\n",
      "TEST:  Loss: 0.58 Acc: 80.34%\n",
      "\n",
      "Epoch: 73/200     LR: 0.04894\n",
      "TRAIN: Loss: 0.80 Acc: 72.42%\n",
      "TEST:  Loss: 0.57 Acc: 80.57%\n",
      "\n",
      "Epoch: 74/200     LR: 0.04878\n",
      "TRAIN: Loss: 0.80 Acc: 72.54%\n",
      "TEST:  Loss: 0.63 Acc: 78.34%\n",
      "\n",
      "Epoch: 75/200     LR: 0.04860\n",
      "TRAIN: Loss: 0.80 Acc: 72.65%\n",
      "TEST:  Loss: 0.65 Acc: 78.19%\n",
      "\n",
      "Epoch: 76/200     LR: 0.04841\n",
      "TRAIN: Loss: 0.79 Acc: 72.86%\n",
      "TEST:  Loss: 0.61 Acc: 79.83%\n",
      "\n",
      "Epoch: 77/200     LR: 0.04820\n",
      "TRAIN: Loss: 0.79 Acc: 72.88%\n",
      "TEST:  Loss: 0.58 Acc: 80.65%\n",
      "\n",
      "Epoch: 78/200     LR: 0.04799\n",
      "TRAIN: Loss: 0.79 Acc: 72.80%\n",
      "TEST:  Loss: 0.82 Acc: 73.96%\n",
      "\n",
      "Epoch: 79/200     LR: 0.04776\n",
      "TRAIN: Loss: 0.79 Acc: 72.85%\n",
      "TEST:  Loss: 0.61 Acc: 78.97%\n",
      "\n",
      "Epoch: 80/200     LR: 0.04753\n",
      "TRAIN: Loss: 0.79 Acc: 72.99%\n",
      "TEST:  Loss: 0.63 Acc: 78.47%\n",
      "\n",
      "Epoch: 81/200     LR: 0.04728\n",
      "TRAIN: Loss: 0.78 Acc: 72.97%\n",
      "TEST:  Loss: 0.63 Acc: 79.01%\n",
      "\n",
      "Epoch: 82/200     LR: 0.04702\n",
      "TRAIN: Loss: 0.78 Acc: 72.97%\n",
      "TEST:  Loss: 0.48 Acc: 83.40%\n",
      "\n",
      "Epoch: 83/200     LR: 0.04674\n",
      "TRAIN: Loss: 0.78 Acc: 73.41%\n",
      "TEST:  Loss: 0.63 Acc: 78.42%\n",
      "\n",
      "Epoch: 84/200     LR: 0.04646\n",
      "TRAIN: Loss: 0.78 Acc: 73.21%\n",
      "TEST:  Loss: 0.56 Acc: 81.18%\n",
      "\n",
      "Epoch: 85/200     LR: 0.04617\n",
      "TRAIN: Loss: 0.78 Acc: 73.22%\n",
      "TEST:  Loss: 0.62 Acc: 79.03%\n",
      "\n",
      "Epoch: 86/200     LR: 0.04587\n",
      "TRAIN: Loss: 0.78 Acc: 73.04%\n",
      "TEST:  Loss: 0.51 Acc: 82.54%\n",
      "\n",
      "Epoch: 87/200     LR: 0.04555\n",
      "TRAIN: Loss: 0.78 Acc: 73.43%\n",
      "TEST:  Loss: 0.59 Acc: 79.79%\n",
      "\n",
      "Epoch: 88/200     LR: 0.04523\n",
      "TRAIN: Loss: 0.77 Acc: 73.42%\n",
      "TEST:  Loss: 0.59 Acc: 80.12%\n",
      "\n",
      "Epoch: 89/200     LR: 0.04489\n",
      "TRAIN: Loss: 0.77 Acc: 73.50%\n",
      "TEST:  Loss: 0.52 Acc: 82.14%\n",
      "\n",
      "Epoch: 90/200     LR: 0.04455\n",
      "TRAIN: Loss: 0.77 Acc: 73.33%\n",
      "TEST:  Loss: 0.62 Acc: 79.24%\n",
      "\n",
      "Epoch: 91/200     LR: 0.04419\n",
      "TRAIN: Loss: 0.77 Acc: 73.56%\n",
      "TEST:  Loss: 0.59 Acc: 80.57%\n",
      "\n",
      "Epoch: 92/200     LR: 0.04383\n",
      "TRAIN: Loss: 0.77 Acc: 73.64%\n",
      "TEST:  Loss: 0.59 Acc: 80.47%\n",
      "\n",
      "Epoch: 93/200     LR: 0.04346\n",
      "TRAIN: Loss: 0.77 Acc: 73.27%\n",
      "TEST:  Loss: 0.49 Acc: 83.24%\n",
      "\n",
      "Epoch: 94/200     LR: 0.04307\n",
      "TRAIN: Loss: 0.77 Acc: 73.71%\n",
      "TEST:  Loss: 0.54 Acc: 81.67%\n",
      "\n",
      "Epoch: 95/200     LR: 0.04268\n",
      "TRAIN: Loss: 0.77 Acc: 73.51%\n",
      "TEST:  Loss: 0.56 Acc: 80.83%\n",
      "\n",
      "Epoch: 96/200     LR: 0.04228\n",
      "TRAIN: Loss: 0.77 Acc: 73.73%\n",
      "TEST:  Loss: 0.47 Acc: 84.16%\n",
      "\n",
      "Epoch: 97/200     LR: 0.04187\n",
      "TRAIN: Loss: 0.77 Acc: 73.64%\n",
      "TEST:  Loss: 0.56 Acc: 81.04%\n",
      "\n",
      "Epoch: 98/200     LR: 0.04145\n",
      "TRAIN: Loss: 0.76 Acc: 73.84%\n",
      "TEST:  Loss: 0.50 Acc: 82.72%\n",
      "\n",
      "Epoch: 99/200     LR: 0.04103\n",
      "TRAIN: Loss: 0.75 Acc: 74.17%\n",
      "TEST:  Loss: 0.50 Acc: 83.10%\n",
      "\n",
      "Epoch: 100/200    LR: 0.04059\n",
      "TRAIN: Loss: 0.76 Acc: 74.15%\n",
      "TEST:  Loss: 0.59 Acc: 80.30%\n",
      "\n",
      "Epoch: 101/200    LR: 0.04015\n",
      "TRAIN: Loss: 0.76 Acc: 73.92%\n",
      "TEST:  Loss: 0.53 Acc: 82.32%\n",
      "\n",
      "Epoch: 102/200    LR: 0.03970\n",
      "TRAIN: Loss: 0.76 Acc: 74.07%\n",
      "TEST:  Loss: 0.50 Acc: 83.23%\n",
      "\n",
      "Epoch: 103/200    LR: 0.03924\n",
      "TRAIN: Loss: 0.76 Acc: 74.04%\n",
      "TEST:  Loss: 0.56 Acc: 81.80%\n",
      "\n",
      "Epoch: 104/200    LR: 0.03878\n",
      "TRAIN: Loss: 0.75 Acc: 74.18%\n",
      "TEST:  Loss: 0.52 Acc: 82.41%\n",
      "\n",
      "Epoch: 105/200    LR: 0.03831\n",
      "TRAIN: Loss: 0.75 Acc: 74.19%\n",
      "TEST:  Loss: 0.54 Acc: 81.97%\n",
      "\n",
      "Epoch: 106/200    LR: 0.03783\n",
      "TRAIN: Loss: 0.75 Acc: 74.31%\n",
      "TEST:  Loss: 0.62 Acc: 79.97%\n",
      "\n",
      "Epoch: 107/200    LR: 0.03734\n",
      "TRAIN: Loss: 0.75 Acc: 74.30%\n",
      "TEST:  Loss: 0.55 Acc: 81.23%\n",
      "\n",
      "Epoch: 108/200    LR: 0.03685\n",
      "TRAIN: Loss: 0.75 Acc: 74.36%\n",
      "TEST:  Loss: 0.64 Acc: 79.35%\n",
      "\n",
      "Epoch: 109/200    LR: 0.03636\n",
      "TRAIN: Loss: 0.75 Acc: 74.47%\n",
      "TEST:  Loss: 0.54 Acc: 81.29%\n",
      "\n",
      "Epoch: 110/200    LR: 0.03585\n",
      "TRAIN: Loss: 0.75 Acc: 74.09%\n",
      "TEST:  Loss: 0.60 Acc: 80.67%\n",
      "\n",
      "Epoch: 111/200    LR: 0.03534\n",
      "TRAIN: Loss: 0.74 Acc: 74.57%\n",
      "TEST:  Loss: 0.57 Acc: 81.12%\n",
      "\n",
      "Epoch: 112/200    LR: 0.03483\n",
      "TRAIN: Loss: 0.74 Acc: 74.33%\n",
      "TEST:  Loss: 0.71 Acc: 76.47%\n",
      "\n",
      "Epoch: 113/200    LR: 0.03431\n",
      "TRAIN: Loss: 0.74 Acc: 74.48%\n",
      "TEST:  Loss: 0.53 Acc: 81.90%\n",
      "\n",
      "Epoch: 114/200    LR: 0.03379\n",
      "TRAIN: Loss: 0.73 Acc: 74.66%\n",
      "TEST:  Loss: 0.65 Acc: 78.99%\n",
      "\n",
      "Epoch: 115/200    LR: 0.03326\n",
      "TRAIN: Loss: 0.73 Acc: 74.66%\n",
      "TEST:  Loss: 0.53 Acc: 82.52%\n",
      "\n",
      "Epoch: 116/200    LR: 0.03273\n",
      "TRAIN: Loss: 0.73 Acc: 74.89%\n",
      "TEST:  Loss: 0.46 Acc: 84.08%\n",
      "\n",
      "Epoch: 117/200    LR: 0.03220\n",
      "TRAIN: Loss: 0.74 Acc: 74.62%\n",
      "TEST:  Loss: 0.49 Acc: 82.89%\n",
      "\n",
      "Epoch: 118/200    LR: 0.03166\n",
      "TRAIN: Loss: 0.74 Acc: 74.78%\n",
      "TEST:  Loss: 0.47 Acc: 84.03%\n",
      "\n",
      "Epoch: 119/200    LR: 0.03112\n",
      "TRAIN: Loss: 0.73 Acc: 74.70%\n",
      "TEST:  Loss: 0.57 Acc: 80.64%\n",
      "\n",
      "Epoch: 120/200    LR: 0.03057\n",
      "TRAIN: Loss: 0.72 Acc: 75.10%\n",
      "TEST:  Loss: 0.48 Acc: 83.26%\n",
      "\n",
      "Epoch: 121/200    LR: 0.03002\n",
      "TRAIN: Loss: 0.73 Acc: 75.07%\n",
      "TEST:  Loss: 0.55 Acc: 81.71%\n",
      "\n",
      "Epoch: 122/200    LR: 0.02947\n",
      "TRAIN: Loss: 0.72 Acc: 75.55%\n",
      "TEST:  Loss: 0.47 Acc: 84.16%\n",
      "\n",
      "Epoch: 123/200    LR: 0.02892\n",
      "TRAIN: Loss: 0.72 Acc: 75.20%\n",
      "TEST:  Loss: 0.45 Acc: 84.49%\n",
      "\n",
      "Epoch: 124/200    LR: 0.02836\n",
      "TRAIN: Loss: 0.73 Acc: 74.98%\n",
      "TEST:  Loss: 0.60 Acc: 79.78%\n",
      "\n",
      "Epoch: 125/200    LR: 0.02781\n",
      "TRAIN: Loss: 0.72 Acc: 75.14%\n",
      "TEST:  Loss: 0.51 Acc: 82.82%\n",
      "\n",
      "Epoch: 126/200    LR: 0.02725\n",
      "TRAIN: Loss: 0.72 Acc: 75.32%\n",
      "TEST:  Loss: 0.46 Acc: 84.61%\n",
      "\n",
      "Epoch: 127/200    LR: 0.02669\n",
      "TRAIN: Loss: 0.72 Acc: 75.36%\n",
      "TEST:  Loss: 0.46 Acc: 84.47%\n",
      "\n",
      "Epoch: 128/200    LR: 0.02613\n",
      "TRAIN: Loss: 0.71 Acc: 75.56%\n",
      "TEST:  Loss: 0.52 Acc: 82.96%\n",
      "\n",
      "Epoch: 129/200    LR: 0.02557\n",
      "TRAIN: Loss: 0.70 Acc: 75.63%\n",
      "TEST:  Loss: 0.52 Acc: 82.57%\n",
      "\n",
      "Epoch: 130/200    LR: 0.02501\n",
      "TRAIN: Loss: 0.71 Acc: 75.43%\n",
      "TEST:  Loss: 0.59 Acc: 80.82%\n",
      "\n",
      "Epoch: 131/200    LR: 0.02445\n",
      "TRAIN: Loss: 0.70 Acc: 75.82%\n",
      "TEST:  Loss: 0.56 Acc: 82.17%\n",
      "\n",
      "Epoch: 132/200    LR: 0.02389\n",
      "TRAIN: Loss: 0.69 Acc: 76.25%\n",
      "TEST:  Loss: 0.43 Acc: 85.76%\n",
      "\n",
      "Epoch: 133/200    LR: 0.02333\n",
      "TRAIN: Loss: 0.70 Acc: 75.97%\n",
      "TEST:  Loss: 0.43 Acc: 85.30%\n",
      "\n",
      "Epoch: 134/200    LR: 0.02277\n",
      "TRAIN: Loss: 0.70 Acc: 76.04%\n",
      "TEST:  Loss: 0.41 Acc: 85.91%\n",
      "\n",
      "Epoch: 135/200    LR: 0.02221\n",
      "TRAIN: Loss: 0.70 Acc: 75.80%\n",
      "TEST:  Loss: 0.46 Acc: 84.30%\n",
      "\n",
      "Epoch: 136/200    LR: 0.02166\n",
      "TRAIN: Loss: 0.69 Acc: 76.46%\n",
      "TEST:  Loss: 0.45 Acc: 84.66%\n",
      "\n",
      "Epoch: 137/200    LR: 0.02110\n",
      "TRAIN: Loss: 0.69 Acc: 76.39%\n",
      "TEST:  Loss: 0.51 Acc: 83.02%\n",
      "\n",
      "Epoch: 138/200    LR: 0.02055\n",
      "TRAIN: Loss: 0.68 Acc: 76.43%\n",
      "TEST:  Loss: 0.40 Acc: 86.33%\n",
      "\n",
      "Epoch: 139/200    LR: 0.02000\n",
      "TRAIN: Loss: 0.67 Acc: 76.87%\n",
      "TEST:  Loss: 0.42 Acc: 85.14%\n",
      "\n",
      "Epoch: 140/200    LR: 0.01945\n",
      "TRAIN: Loss: 0.67 Acc: 76.93%\n",
      "TEST:  Loss: 0.51 Acc: 82.93%\n",
      "\n",
      "Epoch: 141/200    LR: 0.01890\n",
      "TRAIN: Loss: 0.67 Acc: 77.08%\n",
      "TEST:  Loss: 0.43 Acc: 85.82%\n",
      "\n",
      "Epoch: 142/200    LR: 0.01836\n",
      "TRAIN: Loss: 0.67 Acc: 76.89%\n",
      "TEST:  Loss: 0.46 Acc: 84.32%\n",
      "\n",
      "Epoch: 143/200    LR: 0.01782\n",
      "TRAIN: Loss: 0.67 Acc: 76.94%\n",
      "TEST:  Loss: 0.38 Acc: 86.85%\n",
      "\n",
      "Epoch: 144/200    LR: 0.01729\n",
      "TRAIN: Loss: 0.66 Acc: 77.10%\n",
      "TEST:  Loss: 0.40 Acc: 86.52%\n",
      "\n",
      "Epoch: 145/200    LR: 0.01676\n",
      "TRAIN: Loss: 0.66 Acc: 77.20%\n",
      "TEST:  Loss: 0.41 Acc: 86.45%\n",
      "\n",
      "Epoch: 146/200    LR: 0.01623\n",
      "TRAIN: Loss: 0.65 Acc: 77.39%\n",
      "TEST:  Loss: 0.45 Acc: 84.86%\n",
      "\n",
      "Epoch: 147/200    LR: 0.01571\n",
      "TRAIN: Loss: 0.65 Acc: 77.49%\n",
      "TEST:  Loss: 0.40 Acc: 86.58%\n",
      "\n",
      "Epoch: 148/200    LR: 0.01519\n",
      "TRAIN: Loss: 0.64 Acc: 77.96%\n",
      "TEST:  Loss: 0.40 Acc: 86.55%\n",
      "\n",
      "Epoch: 149/200    LR: 0.01468\n",
      "TRAIN: Loss: 0.64 Acc: 78.14%\n",
      "TEST:  Loss: 0.43 Acc: 85.49%\n",
      "\n",
      "Epoch: 150/200    LR: 0.01417\n",
      "TRAIN: Loss: 0.64 Acc: 78.10%\n",
      "TEST:  Loss: 0.36 Acc: 87.98%\n",
      "\n",
      "Epoch: 151/200    LR: 0.01366\n",
      "TRAIN: Loss: 0.64 Acc: 78.23%\n",
      "TEST:  Loss: 0.36 Acc: 87.86%\n",
      "\n",
      "Epoch: 152/200    LR: 0.01317\n",
      "TRAIN: Loss: 0.62 Acc: 78.72%\n",
      "TEST:  Loss: 0.37 Acc: 87.37%\n",
      "\n",
      "Epoch: 153/200    LR: 0.01268\n",
      "TRAIN: Loss: 0.62 Acc: 78.57%\n",
      "TEST:  Loss: 0.40 Acc: 86.15%\n",
      "\n",
      "Epoch: 154/200    LR: 0.01219\n",
      "TRAIN: Loss: 0.62 Acc: 78.64%\n",
      "TEST:  Loss: 0.33 Acc: 88.79%\n",
      "\n",
      "Epoch: 155/200    LR: 0.01171\n",
      "TRAIN: Loss: 0.61 Acc: 78.75%\n",
      "TEST:  Loss: 0.37 Acc: 87.50%\n",
      "\n",
      "Epoch: 156/200    LR: 0.01124\n",
      "TRAIN: Loss: 0.61 Acc: 78.98%\n",
      "TEST:  Loss: 0.39 Acc: 86.98%\n",
      "\n",
      "Epoch: 157/200    LR: 0.01078\n",
      "TRAIN: Loss: 0.60 Acc: 79.31%\n",
      "TEST:  Loss: 0.37 Acc: 87.88%\n",
      "\n",
      "Epoch: 158/200    LR: 0.01032\n",
      "TRAIN: Loss: 0.60 Acc: 79.33%\n",
      "TEST:  Loss: 0.33 Acc: 88.60%\n",
      "\n",
      "Epoch: 159/200    LR: 0.00987\n",
      "TRAIN: Loss: 0.60 Acc: 79.43%\n",
      "TEST:  Loss: 0.35 Acc: 87.80%\n",
      "\n",
      "Epoch: 160/200    LR: 0.00943\n",
      "TRAIN: Loss: 0.59 Acc: 79.93%\n",
      "TEST:  Loss: 0.36 Acc: 87.79%\n",
      "\n",
      "Epoch: 161/200    LR: 0.00899\n",
      "TRAIN: Loss: 0.58 Acc: 79.87%\n",
      "TEST:  Loss: 0.38 Acc: 87.33%\n",
      "\n",
      "Epoch: 162/200    LR: 0.00857\n",
      "TRAIN: Loss: 0.57 Acc: 80.37%\n",
      "TEST:  Loss: 0.33 Acc: 88.58%\n",
      "\n",
      "Epoch: 163/200    LR: 0.00815\n",
      "TRAIN: Loss: 0.57 Acc: 80.31%\n",
      "TEST:  Loss: 0.33 Acc: 88.68%\n",
      "\n",
      "Epoch: 164/200    LR: 0.00774\n",
      "TRAIN: Loss: 0.55 Acc: 80.96%\n",
      "TEST:  Loss: 0.36 Acc: 87.75%\n",
      "\n",
      "Epoch: 165/200    LR: 0.00734\n",
      "TRAIN: Loss: 0.56 Acc: 80.86%\n",
      "TEST:  Loss: 0.33 Acc: 88.90%\n",
      "\n",
      "Epoch: 166/200    LR: 0.00695\n",
      "TRAIN: Loss: 0.55 Acc: 80.92%\n",
      "TEST:  Loss: 0.31 Acc: 89.69%\n",
      "\n",
      "Epoch: 167/200    LR: 0.00656\n",
      "TRAIN: Loss: 0.54 Acc: 81.30%\n",
      "TEST:  Loss: 0.30 Acc: 89.82%\n",
      "\n",
      "Epoch: 168/200    LR: 0.00619\n",
      "TRAIN: Loss: 0.53 Acc: 81.67%\n",
      "TEST:  Loss: 0.31 Acc: 89.37%\n",
      "\n",
      "Epoch: 169/200    LR: 0.00583\n",
      "TRAIN: Loss: 0.52 Acc: 82.34%\n",
      "TEST:  Loss: 0.31 Acc: 89.72%\n",
      "\n",
      "Epoch: 170/200    LR: 0.00547\n",
      "TRAIN: Loss: 0.52 Acc: 82.20%\n",
      "TEST:  Loss: 0.27 Acc: 90.49%\n",
      "\n",
      "Epoch: 171/200    LR: 0.00513\n",
      "TRAIN: Loss: 0.50 Acc: 82.69%\n",
      "TEST:  Loss: 0.28 Acc: 90.31%\n",
      "\n",
      "Epoch: 172/200    LR: 0.00479\n",
      "TRAIN: Loss: 0.50 Acc: 82.80%\n",
      "TEST:  Loss: 0.31 Acc: 89.08%\n",
      "\n",
      "Epoch: 173/200    LR: 0.00447\n",
      "TRAIN: Loss: 0.49 Acc: 82.97%\n",
      "TEST:  Loss: 0.29 Acc: 90.38%\n",
      "\n",
      "Epoch: 174/200    LR: 0.00415\n",
      "TRAIN: Loss: 0.48 Acc: 83.23%\n",
      "TEST:  Loss: 0.30 Acc: 89.71%\n",
      "\n",
      "Epoch: 175/200    LR: 0.00385\n",
      "TRAIN: Loss: 0.48 Acc: 83.60%\n",
      "TEST:  Loss: 0.26 Acc: 90.89%\n",
      "\n",
      "Epoch: 176/200    LR: 0.00356\n",
      "TRAIN: Loss: 0.46 Acc: 84.04%\n",
      "TEST:  Loss: 0.26 Acc: 91.26%\n",
      "\n",
      "Epoch: 177/200    LR: 0.00328\n",
      "TRAIN: Loss: 0.46 Acc: 84.23%\n",
      "TEST:  Loss: 0.25 Acc: 91.50%\n",
      "\n",
      "Epoch: 178/200    LR: 0.00300\n",
      "TRAIN: Loss: 0.45 Acc: 84.42%\n",
      "TEST:  Loss: 0.24 Acc: 91.65%\n",
      "\n",
      "Epoch: 179/200    LR: 0.00274\n",
      "TRAIN: Loss: 0.44 Acc: 84.95%\n",
      "TEST:  Loss: 0.25 Acc: 91.69%\n",
      "\n",
      "Epoch: 180/200    LR: 0.00249\n",
      "TRAIN: Loss: 0.42 Acc: 85.57%\n",
      "TEST:  Loss: 0.23 Acc: 92.12%\n",
      "\n",
      "Epoch: 181/200    LR: 0.00226\n",
      "TRAIN: Loss: 0.41 Acc: 85.84%\n",
      "TEST:  Loss: 0.23 Acc: 92.59%\n",
      "\n",
      "Epoch: 182/200    LR: 0.00203\n",
      "TRAIN: Loss: 0.40 Acc: 85.95%\n",
      "TEST:  Loss: 0.22 Acc: 92.73%\n",
      "\n",
      "Epoch: 183/200    LR: 0.00182\n",
      "TRAIN: Loss: 0.40 Acc: 86.31%\n",
      "TEST:  Loss: 0.22 Acc: 92.61%\n",
      "\n",
      "Epoch: 184/200    LR: 0.00161\n",
      "TRAIN: Loss: 0.39 Acc: 86.62%\n",
      "TEST:  Loss: 0.21 Acc: 93.25%\n",
      "\n",
      "Epoch: 185/200    LR: 0.00142\n",
      "TRAIN: Loss: 0.38 Acc: 87.02%\n",
      "TEST:  Loss: 0.22 Acc: 92.80%\n",
      "\n",
      "Epoch: 186/200    LR: 0.00124\n",
      "TRAIN: Loss: 0.37 Acc: 87.41%\n",
      "TEST:  Loss: 0.21 Acc: 92.96%\n",
      "\n",
      "Epoch: 187/200    LR: 0.00108\n",
      "TRAIN: Loss: 0.36 Acc: 87.75%\n",
      "TEST:  Loss: 0.20 Acc: 93.04%\n",
      "\n",
      "Epoch: 188/200    LR: 0.00092\n",
      "TRAIN: Loss: 0.34 Acc: 88.14%\n",
      "TEST:  Loss: 0.19 Acc: 93.81%\n",
      "\n",
      "Epoch: 189/200    LR: 0.00078\n",
      "TRAIN: Loss: 0.34 Acc: 88.22%\n",
      "TEST:  Loss: 0.19 Acc: 93.57%\n",
      "\n",
      "Epoch: 190/200    LR: 0.00065\n",
      "TRAIN: Loss: 0.32 Acc: 88.91%\n",
      "TEST:  Loss: 0.18 Acc: 94.01%\n",
      "\n",
      "Epoch: 191/200    LR: 0.00053\n",
      "TRAIN: Loss: 0.32 Acc: 89.03%\n",
      "TEST:  Loss: 0.18 Acc: 94.16%\n",
      "\n",
      "Epoch: 192/200    LR: 0.00042\n",
      "TRAIN: Loss: 0.32 Acc: 89.17%\n",
      "TEST:  Loss: 0.18 Acc: 94.02%\n",
      "\n",
      "Epoch: 193/200    LR: 0.00033\n",
      "TRAIN: Loss: 0.31 Acc: 89.46%\n",
      "TEST:  Loss: 0.18 Acc: 93.94%\n",
      "\n",
      "Epoch: 194/200    LR: 0.00025\n",
      "TRAIN: Loss: 0.31 Acc: 89.31%\n",
      "TEST:  Loss: 0.18 Acc: 93.92%\n",
      "\n",
      "Epoch: 195/200    LR: 0.00018\n",
      "TRAIN: Loss: 0.30 Acc: 89.67%\n",
      "TEST:  Loss: 0.17 Acc: 94.26%\n",
      "\n",
      "Epoch: 196/200    LR: 0.00012\n",
      "TRAIN: Loss: 0.30 Acc: 89.93%\n",
      "TEST:  Loss: 0.17 Acc: 94.17%\n",
      "\n",
      "Epoch: 197/200    LR: 0.00008\n",
      "TRAIN: Loss: 0.29 Acc: 89.97%\n",
      "TEST:  Loss: 0.17 Acc: 94.17%\n",
      "\n",
      "Epoch: 198/200    LR: 0.00005\n",
      "TRAIN: Loss: 0.29 Acc: 90.07%\n",
      "TEST:  Loss: 0.17 Acc: 94.21%\n",
      "\n",
      "Epoch: 199/200    LR: 0.00003\n",
      "TRAIN: Loss: 0.29 Acc: 90.17%\n",
      "TEST:  Loss: 0.17 Acc: 94.26%\n",
      "\n",
      "Epoch: 200/200    LR: 0.00002\n",
      "TRAIN: Loss: 0.29 Acc: 90.14%\n",
      "TEST:  Loss: 0.17 Acc: 94.22%\n",
      "Model saved as ResNetCustom_epoch195_acc94.pth\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "try:\n",
    "    # Define optimizer\n",
    "    OPTIMIZER = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=8e-4)\n",
    "    \n",
    "    # Define scheduler\n",
    "    SCHEDULER = optim.lr_scheduler.OneCycleLR(OPTIMIZER, max_lr=0.05, total_steps=EPOCHS, final_div_factor=100)\n",
    "    \n",
    "    # If no scheduler, set to None\n",
    "    SCHEDULER = SCHEDULER if 'SCHEDULER' in locals() and SCHEDULER is not None else None\n",
    "\n",
    "    # train(model, EPOCHS)\n",
    "    train(model, EPOCHS, train_batch_size=TRAIN_BATCH_SIZE, test_batch_size=TEST_BATCH_SIZE, augmentations=AUGMENTATIONS,\n",
    "          optimizer=OPTIMIZER, scheduler=SCHEDULER, save=SAVE_MODE, every_n=SAVE_EVERY_N)\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(f'Training failed: {e}')\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T03:19:39.321666Z",
     "iopub.status.busy": "2025-03-10T03:19:39.321290Z",
     "iopub.status.idle": "2025-03-10T03:19:43.331342Z",
     "shell.execute_reply": "2025-03-10T03:19:43.330601Z",
     "shell.execute_reply.started": "2025-03-10T03:19:39.321635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading model...\n",
      "Running inference...\n",
      "Predictions saved as ResNetCustom_epoch195_acc94.csv\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "try:\n",
    "    inference('ResNetCustom_epoch195_acc94.pth')\n",
    "except FileNotFoundError as e:\n",
    "    print(f'Inference failed: {e}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11145869,
     "sourceId": 93057,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
